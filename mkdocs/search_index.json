{
    "docs": [
        {
            "location": "/", 
            "text": "Netscaler Ansible Docs\n\n\nThis project implements a set of Ansible modules for the Citrix Netscaler. Users of these modules can create, edit, update, and delete configuration objects on a Netscaler. For more information on the basic principals that the modules use, see the usage/index.\n\n\nThe code is licensed under the GPL and the authoritative repository is on github\n\n\nThe main documentation for the modules is organized into several sections listed below.\n\n\nUser Documentation\n\n\nGetting Started\nInstalling Ansible\nInstalling Modules\nPlaybook\nSpeeding up execution\nSaving configuration\nSample playbook\nClosing remarks\nRolling upgrades\nSetup\nTestbed\nUpgrade process\nReferences\nRolling upgrades (VPX)\nSetup\nInitializing the testbed\nUpgrade process\nReferences\nNetscaler ansible docker image\nInstallation\nUsage\nExample\n\n\nUsing generic ansible modules\n\n\nUsing generic Ansible modules\nReferences\nTemplating the configuration file\nWorkflow\nPlaybook\nReferences\nDirect NITRO API calls\nWorkflow\nPlaybook\nReferences\n\n\nModule Documentation\n\n\nModule Index\nAll Modules\nNetwork Modules\n\n\nDeveloper Documentation\n\n\nDevelopment Utilities\nDeveloping a new module\nGetting the spec of a nitro object\nGenerating the boilerplate", 
            "title": "NetScaler Ansible Docs"
        }, 
        {
            "location": "/#netscaler-ansible-docs", 
            "text": "This project implements a set of Ansible modules for the Citrix Netscaler. Users of these modules can create, edit, update, and delete configuration objects on a Netscaler. For more information on the basic principals that the modules use, see the usage/index.  The code is licensed under the GPL and the authoritative repository is on github  The main documentation for the modules is organized into several sections listed below.", 
            "title": "Netscaler Ansible Docs"
        }, 
        {
            "location": "/#user-documentation", 
            "text": "Getting Started\nInstalling Ansible\nInstalling Modules\nPlaybook\nSpeeding up execution\nSaving configuration\nSample playbook\nClosing remarks\nRolling upgrades\nSetup\nTestbed\nUpgrade process\nReferences\nRolling upgrades (VPX)\nSetup\nInitializing the testbed\nUpgrade process\nReferences\nNetscaler ansible docker image\nInstallation\nUsage\nExample", 
            "title": "User Documentation"
        }, 
        {
            "location": "/#using-generic-ansible-modules", 
            "text": "Using generic Ansible modules\nReferences\nTemplating the configuration file\nWorkflow\nPlaybook\nReferences\nDirect NITRO API calls\nWorkflow\nPlaybook\nReferences", 
            "title": "Using generic ansible modules"
        }, 
        {
            "location": "/#module-documentation", 
            "text": "Module Index\nAll Modules\nNetwork Modules", 
            "title": "Module Documentation"
        }, 
        {
            "location": "/#developer-documentation", 
            "text": "Development Utilities\nDeveloping a new module\nGetting the spec of a nitro object\nGenerating the boilerplate", 
            "title": "Developer Documentation"
        }, 
        {
            "location": "/usage/getting-started/", 
            "text": "Getting Started\n\n\nThis document will show you how to begin using the Netscaler Ansible modules.\n\n\nFirst, obtain \nPython 2.7\n and \nAnsible\n if you do not already have them.\n\n\nThe version of Ansible that is required is at least 2.4.0.\n\n\nInstalling Ansible\n\n\nInstalling Ansible may be accomplished through the following methods.\n\n\nFurther documentation on installing Ansible  may be found in \ngithub\n\n\nUsing pip\n\n\npip install ansible\n\n\n\n\n\nUsing your package manager\n\n\nE.g. in a Debian based Linux distribution\n\n\napt-get install ansible\n\n\n\n\n\nUsing a direct checkout\n\n\ngit clone https://github.com/ansible/ansible\n\n\ncd\n ansible\n\n\nsource\n hacking/env-setup\n\n\n\n\n\nVerifying the installation\n\n\nFollowing any installation method you should be able to run the following code which will print out the ansible version you will be using\n\n\nansible --version\n\n\n\n\n\nInstalling Modules\n\n\nTo install the latest version of the Netscaler modules run the following commands\n\n\ngit clone https://github.com/citrix/netscaler-ansible-modules\n\n\ncd\n netscaler-ansible-modules\n\npython install.py\n\n\n\n\n\nThe install script will detect where the ansible library is installed and will try to copy the module files to the appropriate directories.\n\n\n\n\nNote\nThe last step may require root priviledges depending on where ansible is installed.\n\n\n\n\n\n\nPlaybook\n\n\nLast we are going to see how to make a simple playbook. \n\n\n   \n-\n \nname\n:\n \nCreate a server\n\n       \ndelegate_to\n:\n \nlocalhost\n\n       \ngather_facts\n:\n \nno\n\n\n       \nnetscaler_server\n:\n\n           \nnsip\n:\n \n172.18.0.2\n\n           \nnitro_user\n:\n \nnsroot\n\n           \nnitro_pass\n:\n \nnsroot\n\n\n           \nstate\n:\n \npresent\n\n\n           \nname\n:\n \ntest-server-1\n\n           \nipaddress\n:\n \n192.168.1.1", 
            "title": "Getting Started"
        }, 
        {
            "location": "/usage/getting-started/#getting-started", 
            "text": "This document will show you how to begin using the Netscaler Ansible modules.  First, obtain  Python 2.7  and  Ansible  if you do not already have them.  The version of Ansible that is required is at least 2.4.0.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/usage/getting-started/#installing-ansible", 
            "text": "Installing Ansible may be accomplished through the following methods.  Further documentation on installing Ansible  may be found in  github", 
            "title": "Installing Ansible"
        }, 
        {
            "location": "/usage/getting-started/#using-pip", 
            "text": "pip install ansible", 
            "title": "Using pip"
        }, 
        {
            "location": "/usage/getting-started/#using-your-package-manager", 
            "text": "E.g. in a Debian based Linux distribution  apt-get install ansible", 
            "title": "Using your package manager"
        }, 
        {
            "location": "/usage/getting-started/#using-a-direct-checkout", 
            "text": "git clone https://github.com/ansible/ansible cd  ansible source  hacking/env-setup", 
            "title": "Using a direct checkout"
        }, 
        {
            "location": "/usage/getting-started/#verifying-the-installation", 
            "text": "Following any installation method you should be able to run the following code which will print out the ansible version you will be using  ansible --version", 
            "title": "Verifying the installation"
        }, 
        {
            "location": "/usage/getting-started/#installing-modules", 
            "text": "To install the latest version of the Netscaler modules run the following commands  git clone https://github.com/citrix/netscaler-ansible-modules cd  netscaler-ansible-modules\n\npython install.py  The install script will detect where the ansible library is installed and will try to copy the module files to the appropriate directories.   Note The last step may require root priviledges depending on where ansible is installed.", 
            "title": "Installing Modules"
        }, 
        {
            "location": "/usage/getting-started/#playbook", 
            "text": "Last we are going to see how to make a simple playbook.       -   name :   Create a server \n        delegate_to :   localhost \n        gather_facts :   no \n\n        netscaler_server : \n            nsip :   172.18.0.2 \n            nitro_user :   nsroot \n            nitro_pass :   nsroot \n\n            state :   present \n\n            name :   test-server-1 \n            ipaddress :   192.168.1.1", 
            "title": "Playbook"
        }, 
        {
            "location": "/usage/speeding-up-execution/", 
            "text": "Speeding up execution\n\n\nThis document details how to speed up the execution time of a playbook\ncontaining invocations of Netscaler ansible modules.\n\n\nAnsible has some options to help with speeding up execution by making\nforks of itself to execute playbooks in multiple target hosts.\n\n\nAlso in the context of a single playbook the use of the \nasync\n keyword\nmay help with parallelizing the execution of the tasks therein but at\nthe cost of increased complexity.\n\n\nBoth of the above methods can be used to speed up the execution of\nplaybooks containing invocations of Netscaler modules.\n\n\nHere we will detail a third option which is specific to the Netscaler\nmodules and the way the underlying API is used.\n\n\nSaving configuration\n\n\nBy default every Netscaler module after it performs any changes to the configuration\nof the Netscaler node will also save the configuration.\n\n\nWhile this is the safest option as far as robustness is concerned, it turns out that the save configuration operation\nis quite costly time wise, taking up to 5 seconds.\n\n\nWhen multiple tasks within a playbook make changes to Netscaler entities these\ndelays accumulate to a substantial amount.\n\n\nThe solution is to instruct the Netscaler modules not to save the configuration\nindividually but instead notify a handler which will save the configuration once\nat the end of the playbook execution.\n\n\nTo do this we need to use the \nsave_config\n option along with the \nnetscaler_save_config\n\nmodule which will be invoked by the handler.\n\n\nSample playbook\n\n\nThe following playbook demonstrates this technique.\n\n\n        \n-\n \nhosts\n:\n \nnetscaler\n\n\n          \nvars\n:\n\n            \nsave_config\n:\n \nno\n\n            \nstate\n:\n \npresent\n\n\n          \ntasks\n:\n\n            \n-\n \nname\n:\n \nSetup server 1\n\n\n              \ndelegate_to\n:\n \nlocalhost\n\n              \nnotify\n:\n \nSave netscaler configuration\n\n\n              \nnetscaler_server\n:\n\n                \nnsip\n:\n \n172.18.0.2\n\n                \nnitro_user\n:\n \nnsroot\n\n                \nnitro_pass\n:\n \nnsroot\n\n\n                \nstate\n:\n \n{{\n \nstate\n \n}}\n\n                \nsave_config\n:\n \n{{\n \nsave_config\n \n}}\n\n\n                \nname\n:\n \nserver-1\n\n                \nipaddress\n:\n \n192.168.1.1\n\n                \ncomment\n:\n \nOur first server\n\n\n            \n-\n \nname\n:\n \nSet server 2\n\n\n              \ndelegate_to\n:\n \nlocalhost\n\n              \nnotify\n:\n \nSave netscaler configuration\n\n\n              \nnetscaler_server\n:\n\n                \nnsip\n:\n \n172.18.0.2\n\n                \nnitro_user\n:\n \nnsroot\n\n                \nnitro_pass\n:\n \nnsroot\n\n\n                \nstate\n:\n \n{{\n \nstate\n \n}}\n\n                \nsave_config\n:\n \n{{\n \nsave_config\n \n}}\n\n\n                \nname\n:\n \nserver-2\n\n                \nipaddress\n:\n \n192.168.1.2\n\n                \ncomment\n:\n \nOur second server\n\n\n          \nhandlers\n:\n\n            \n-\n \nname\n:\n \nSave netscaler configuration\n\n              \ndelegate_to\n:\n \nlocalhost\n\n              \nnetscaler_save_config\n:\n\n                \nnsip\n:\n \n172.18.0.2\n\n                \nnitro_user\n:\n \nnsroot\n\n                \nnitro_pass\n:\n \nnsroot\n\n\n\n\n\n\nClosing remarks\n\n\nAs you see in the example we need to explicitly set the \nsave_config\n option\nsince by default it is set to \nyes\n.\n\n\nAlso we call the \nnetscaler_save_config\n module only once in the handlers section.\n\n\nThe number of times the configuration will be saved on the Netscaler module is\nonly one regardless of the number of changes, or none if there is no change recorded\nin the result of any of the netscaler modules.\n\n\nThis is much better than the worst case with the default \nsave_config\n option which would\nsave the configuration twice if both server modules made changes.\n\n\nIt is also just as fast as the best case with the default \nsave_config\n option which would be\nto save the configuration once in case only one of the tasks made any change.\n\n\nAlso note that the potential benefit increases, for each Netscaler module which utilizes the\nsave configuration handler. For example if we had ten Netscaler modules making changes we would\nbe saving the configuration ten times. Instead if these modules use the \nnetscaler_save_config\n\nas a handler we will have only one call to the save operation.", 
            "title": "Speeding up execution"
        }, 
        {
            "location": "/usage/speeding-up-execution/#speeding-up-execution", 
            "text": "This document details how to speed up the execution time of a playbook\ncontaining invocations of Netscaler ansible modules.  Ansible has some options to help with speeding up execution by making\nforks of itself to execute playbooks in multiple target hosts.  Also in the context of a single playbook the use of the  async  keyword\nmay help with parallelizing the execution of the tasks therein but at\nthe cost of increased complexity.  Both of the above methods can be used to speed up the execution of\nplaybooks containing invocations of Netscaler modules.  Here we will detail a third option which is specific to the Netscaler\nmodules and the way the underlying API is used.", 
            "title": "Speeding up execution"
        }, 
        {
            "location": "/usage/speeding-up-execution/#saving-configuration", 
            "text": "By default every Netscaler module after it performs any changes to the configuration\nof the Netscaler node will also save the configuration.  While this is the safest option as far as robustness is concerned, it turns out that the save configuration operation\nis quite costly time wise, taking up to 5 seconds.  When multiple tasks within a playbook make changes to Netscaler entities these\ndelays accumulate to a substantial amount.  The solution is to instruct the Netscaler modules not to save the configuration\nindividually but instead notify a handler which will save the configuration once\nat the end of the playbook execution.  To do this we need to use the  save_config  option along with the  netscaler_save_config \nmodule which will be invoked by the handler.", 
            "title": "Saving configuration"
        }, 
        {
            "location": "/usage/speeding-up-execution/#sample-playbook", 
            "text": "The following playbook demonstrates this technique.           -   hosts :   netscaler \n\n           vars : \n             save_config :   no \n             state :   present \n\n           tasks : \n             -   name :   Setup server 1 \n\n               delegate_to :   localhost \n               notify :   Save netscaler configuration \n\n               netscaler_server : \n                 nsip :   172.18.0.2 \n                 nitro_user :   nsroot \n                 nitro_pass :   nsroot \n\n                 state :   {{   state   }} \n                 save_config :   {{   save_config   }} \n\n                 name :   server-1 \n                 ipaddress :   192.168.1.1 \n                 comment :   Our first server \n\n             -   name :   Set server 2 \n\n               delegate_to :   localhost \n               notify :   Save netscaler configuration \n\n               netscaler_server : \n                 nsip :   172.18.0.2 \n                 nitro_user :   nsroot \n                 nitro_pass :   nsroot \n\n                 state :   {{   state   }} \n                 save_config :   {{   save_config   }} \n\n                 name :   server-2 \n                 ipaddress :   192.168.1.2 \n                 comment :   Our second server \n\n           handlers : \n             -   name :   Save netscaler configuration \n               delegate_to :   localhost \n               netscaler_save_config : \n                 nsip :   172.18.0.2 \n                 nitro_user :   nsroot \n                 nitro_pass :   nsroot", 
            "title": "Sample playbook"
        }, 
        {
            "location": "/usage/speeding-up-execution/#closing-remarks", 
            "text": "As you see in the example we need to explicitly set the  save_config  option\nsince by default it is set to  yes .  Also we call the  netscaler_save_config  module only once in the handlers section.  The number of times the configuration will be saved on the Netscaler module is\nonly one regardless of the number of changes, or none if there is no change recorded\nin the result of any of the netscaler modules.  This is much better than the worst case with the default  save_config  option which would\nsave the configuration twice if both server modules made changes.  It is also just as fast as the best case with the default  save_config  option which would be\nto save the configuration once in case only one of the tasks made any change.  Also note that the potential benefit increases, for each Netscaler module which utilizes the\nsave configuration handler. For example if we had ten Netscaler modules making changes we would\nbe saving the configuration ten times. Instead if these modules use the  netscaler_save_config \nas a handler we will have only one call to the save operation.", 
            "title": "Closing remarks"
        }, 
        {
            "location": "/usage/rolling-upgrades/", 
            "text": "Rolling upgrades\n\n\nThis document demonstrates how to to a rolling upgrade with zero\ndowntime for a simple load balanced service.\n\n\nThe methods showcased here are also applicable to more complex\nnetworking setups.\n\n\nSetup\n\n\nThe example utilizes Netscaler CPX and docker images.\n\n\nThe dependencies needed to run the playbooks are at\nthe following \ngithub repository\n\n\nTo create the docker containers from the checkout root run the following\n\ndocker-compose up -d\n.\n\n\nTo setup the cpx image you will need to run the rolling_init.yaml playbook\n\nansible-playbook -i inventory.txt rolling_init.yaml\n\n\nTIP: If you are running this using Docker for Mac or Docker for Windows, then ansible will not be able to reach the NetScaler CPX container on the specified IP in the inventory. You have to change the inventory.txt file to point to the actual port that the CPX API port is mapped to\n\n\ndocker port netscalerrollingupdatesexample_cpx_1\n161/tcp -\n 0.0.0.0:32805\n22/tcp -\n 0.0.0.0:32807\n443/tcp -\n 0.0.0.0:32804\n80/tcp -\n 0.0.0.0:32806\ncat inventory.txt\n    [all:vars]\n    nsip=127.0.0.1:32806\n\n\n\n\n\nThis will initialize the testbed to the topology shown below.\n\n\nTestbed\n\n\nThe testbed is comprised of a Netscaler CPX load balancer and 2 docker containers\nthat act as the backend servers for the load balanced service.\n\n\nThe logical diagram of the testbed is as follows\n\n\n                          +\n                          |\n                          |\n                          |\n                +---------V----------+\n                | Load balancer      |\n                | lb_vserver_1       |----------+\n                | 172.30.0.200:8000  |          |\n                +--------------------+          |\n                        |                       |\n                        |                       |\n                        |                       |\n                +-------V-----+           +-----V-------+\n                | Service 1   |           | Service 2   |\n                | server_1    |           | server_2    |\n                | port 8000   |           | port 8000   |\n                +-------------+           +-------------+\n                        |                       |\n                        |                       |\n                        |                       |\n                +-------V-----+           +-----V-------+\n                | server_1    |           | server_2    |\n                | 172.30.0.21 |           | 172.30.0.22 |\n                +-------------+           +-------------+\n                        |                       |\n                        |                       |\n                        |                       |\n                        |                       |\n                +-------V----------+    +-------V----------+\n                | web server 1     |    | web server 2     |\n                | 172.30.0.21:8000 |    | 172.30.0.22:8000 |\n                +------------------+    +------------------+\n\n\n\n\n\nIn this setup the load balancer virtual server is configured with the\nROUNDROBIN load balancing method and has 2 service members with 50%\nweight each.\n\n\nTo check that the load balancer works correctly run the following command\n\n\ncurl 172.30.0.200:8000\n\n\n\n\n\nYou should see a \nHello webapp1\n.\nRunning the same a second time should output \nHello webapp2\n.\n\n\nTIP: If you are running this example using Docker for Mac or Docker for Windows, the docker network is not visible to your OS. In this case, use another container to execute this curl\n\n\ndocker\n \nrun\n \n--rm\n \n--network\n=\nnetscalerrollingupdatesexample_netscaler\n \n--entrypoint\n \n/bin/sh\n  \nbyrnedo\n/\nalpine-curl\n \n-c\n \nwhile true; do curl  -s http://172.30.0.200:8000; sleep 1; done\n\n\n\n\n\n\nUpgrade process\n\n\nThe upgrade playbook utilizes the \npre_tasks\n and \npost_tasks\n hooks to\nbring the services down and back up during the update process.\n\n\nThe upgrade playbook is the following:\n\n\n        - hosts: webservers\n\n\n\n          remote_user: root\n\n\n          gather_facts: False\n\n\n          serial: 1\n\n\n\n          pre_tasks:\n\n\n            - name: \nDisable \n{{\n \nservername\n \n}}\n\n\n              delegate_to: localhost\n\n\n              netscaler_server:\n\n\n                nsip: \n{{\n \nnsip\n \n}}\n\n\n                nitro_user: \n{{\n \nnitro_user\n \n}}\n\n\n                nitro_pass: \n{{\n \nnitro_pass\n \n}}\n\n\n\n                disabled: yes\n\n\n\n                name: \n{{\n \nservername\n \n}}\n\n\n                ipaddress: \n{{\n \nhostip\n \n}}\n\n\n\n          post_tasks:\n\n\n\n            - name: \nRe enable \n{{\n \nservername\n \n}}\n\n\n              delegate_to: localhost\n\n\n              netscaler_server:\n\n\n                nsip: \n{{\n \nnsip\n \n}}\n\n\n                nitro_user: \n{{\n \nnitro_user\n \n}}\n\n\n                nitro_pass: \n{{\n \nnitro_pass\n \n}}\n\n\n\n                name: \n{{\n \nservername\n \n}}\n\n\n                ipaddress: \n{{\n \nhostip\n \n}}\n\n\n\n          tasks:\n\n\n\n            - name: \nUpdate \n{{\n \nservername\n \n}}\n\n\n              delegate_to: localhost\n\n\n              command: docker-compose exec -d \n{{\n \nservername\n \n}}\n bash -c \necho \nhello updated \n{{\n \nservername\n \n}}\n \n /app/content.txt\n\n\n\n\n\n\nThe function of the pre_tasks and post_tasks hooks is documented by\n\nansible \nhttps://docs.ansible.com/ansible/playbooks_roles.html\n_.\n\n\nEssentially what we do is that we disable the server entity in Netscaler\nfor each web service before the update process and after the update we\nre enable the server entity.\n\n\nThe \nserial: 1\n option instructs ansible to operate on the webservers\none at a time. This is a deviation from the default behavior of Ansible\nwhich is to operate on multiple nodes at once.\n\n\nIn our example the update process is just a simple change of the\ncontent file on the web service docker container to verify\nthe update has taken effect.\n\n\nTo see how the update works you can run\n\n\ncurl 172.30.0.200:8000\n\n\n\n\n\nduring the update process and see how the output changes.\n\n\nSince the update itself is a relatively quick process  you may\nnot be able to see the \nrolling\n nature of the upgrade.\n\n\nFor that you may want to run the update script in step mode\n\n\nansible-playbook -i inventory.txt rolling_update.yml --step\n\n\n\n\n\nand watch the output of\n\n\ncurl 172.30.0.200:8000\n\n\n\n\n\na number of times to actually see what happens.\n\n\nWhat you should see is each server taken out of the load balancing\npool and then brought up without any service interruption.\n\n\nIn our example the update of the web server is instantaneous\nwe do not have any down time.\n\n\nIn a real world situation the update would put the webserver in a\nstate that would be unable to respond to requests.\n\n\nHad we not disabled the corresponding server, in this case, would\nmean that a number of requests would be directed to the offline\nserver resulting in clients getting error responses.\n\n\nEventually the monitors attached to the Netscaler services would\ntake the disrupted service out of the load balancing pool\nbut depending on the traffic volume several requests would have\nbeen affected by the non functioning service by that time.\n\n\nDisabling the server before the update process guarantees that\nNetscaler will not direct any traffic to it during that time,\nensuring continuous delivery of the content.\n\n\nReferences\n\n\n\n\nNetscaler ansible modules repository\n\n\nAnsible documentation", 
            "title": "Rolling upgrades"
        }, 
        {
            "location": "/usage/rolling-upgrades/#rolling-upgrades", 
            "text": "This document demonstrates how to to a rolling upgrade with zero\ndowntime for a simple load balanced service.  The methods showcased here are also applicable to more complex\nnetworking setups.", 
            "title": "Rolling upgrades"
        }, 
        {
            "location": "/usage/rolling-upgrades/#setup", 
            "text": "The example utilizes Netscaler CPX and docker images.  The dependencies needed to run the playbooks are at\nthe following  github repository  To create the docker containers from the checkout root run the following docker-compose up -d .  To setup the cpx image you will need to run the rolling_init.yaml playbook ansible-playbook -i inventory.txt rolling_init.yaml  TIP: If you are running this using Docker for Mac or Docker for Windows, then ansible will not be able to reach the NetScaler CPX container on the specified IP in the inventory. You have to change the inventory.txt file to point to the actual port that the CPX API port is mapped to  docker port netscalerrollingupdatesexample_cpx_1\n161/tcp -  0.0.0.0:32805\n22/tcp -  0.0.0.0:32807\n443/tcp -  0.0.0.0:32804\n80/tcp -  0.0.0.0:32806\ncat inventory.txt\n    [all:vars]\n    nsip=127.0.0.1:32806  This will initialize the testbed to the topology shown below.", 
            "title": "Setup"
        }, 
        {
            "location": "/usage/rolling-upgrades/#testbed", 
            "text": "The testbed is comprised of a Netscaler CPX load balancer and 2 docker containers\nthat act as the backend servers for the load balanced service.  The logical diagram of the testbed is as follows                            +\n                          |\n                          |\n                          |\n                +---------V----------+\n                | Load balancer      |\n                | lb_vserver_1       |----------+\n                | 172.30.0.200:8000  |          |\n                +--------------------+          |\n                        |                       |\n                        |                       |\n                        |                       |\n                +-------V-----+           +-----V-------+\n                | Service 1   |           | Service 2   |\n                | server_1    |           | server_2    |\n                | port 8000   |           | port 8000   |\n                +-------------+           +-------------+\n                        |                       |\n                        |                       |\n                        |                       |\n                +-------V-----+           +-----V-------+\n                | server_1    |           | server_2    |\n                | 172.30.0.21 |           | 172.30.0.22 |\n                +-------------+           +-------------+\n                        |                       |\n                        |                       |\n                        |                       |\n                        |                       |\n                +-------V----------+    +-------V----------+\n                | web server 1     |    | web server 2     |\n                | 172.30.0.21:8000 |    | 172.30.0.22:8000 |\n                +------------------+    +------------------+  In this setup the load balancer virtual server is configured with the\nROUNDROBIN load balancing method and has 2 service members with 50%\nweight each.  To check that the load balancer works correctly run the following command  curl 172.30.0.200:8000  You should see a  Hello webapp1 .\nRunning the same a second time should output  Hello webapp2 .  TIP: If you are running this example using Docker for Mac or Docker for Windows, the docker network is not visible to your OS. In this case, use another container to execute this curl  docker   run   --rm   --network = netscalerrollingupdatesexample_netscaler   --entrypoint   /bin/sh    byrnedo / alpine-curl   -c   while true; do curl  -s http://172.30.0.200:8000; sleep 1; done", 
            "title": "Testbed"
        }, 
        {
            "location": "/usage/rolling-upgrades/#upgrade-process", 
            "text": "The upgrade playbook utilizes the  pre_tasks  and  post_tasks  hooks to\nbring the services down and back up during the update process.  The upgrade playbook is the following:          - hosts: webservers            remote_user: root            gather_facts: False            serial: 1            pre_tasks:              - name:  Disable  {{   servername   }}                delegate_to: localhost                netscaler_server:                  nsip:  {{   nsip   }}                  nitro_user:  {{   nitro_user   }}                  nitro_pass:  {{   nitro_pass   }}                  disabled: yes                  name:  {{   servername   }}                  ipaddress:  {{   hostip   }}            post_tasks:              - name:  Re enable  {{   servername   }}                delegate_to: localhost                netscaler_server:                  nsip:  {{   nsip   }}                  nitro_user:  {{   nitro_user   }}                  nitro_pass:  {{   nitro_pass   }}                  name:  {{   servername   }}                  ipaddress:  {{   hostip   }}            tasks:              - name:  Update  {{   servername   }}                delegate_to: localhost                command: docker-compose exec -d  {{   servername   }}  bash -c  echo  hello updated  {{   servername   }}    /app/content.txt   The function of the pre_tasks and post_tasks hooks is documented by ansible  https://docs.ansible.com/ansible/playbooks_roles.html _.  Essentially what we do is that we disable the server entity in Netscaler\nfor each web service before the update process and after the update we\nre enable the server entity.  The  serial: 1  option instructs ansible to operate on the webservers\none at a time. This is a deviation from the default behavior of Ansible\nwhich is to operate on multiple nodes at once.  In our example the update process is just a simple change of the\ncontent file on the web service docker container to verify\nthe update has taken effect.  To see how the update works you can run  curl 172.30.0.200:8000  during the update process and see how the output changes.  Since the update itself is a relatively quick process  you may\nnot be able to see the  rolling  nature of the upgrade.  For that you may want to run the update script in step mode  ansible-playbook -i inventory.txt rolling_update.yml --step  and watch the output of  curl 172.30.0.200:8000  a number of times to actually see what happens.  What you should see is each server taken out of the load balancing\npool and then brought up without any service interruption.  In our example the update of the web server is instantaneous\nwe do not have any down time.  In a real world situation the update would put the webserver in a\nstate that would be unable to respond to requests.  Had we not disabled the corresponding server, in this case, would\nmean that a number of requests would be directed to the offline\nserver resulting in clients getting error responses.  Eventually the monitors attached to the Netscaler services would\ntake the disrupted service out of the load balancing pool\nbut depending on the traffic volume several requests would have\nbeen affected by the non functioning service by that time.  Disabling the server before the update process guarantees that\nNetscaler will not direct any traffic to it during that time,\nensuring continuous delivery of the content.", 
            "title": "Upgrade process"
        }, 
        {
            "location": "/usage/rolling-upgrades/#references", 
            "text": "Netscaler ansible modules repository  Ansible documentation", 
            "title": "References"
        }, 
        {
            "location": "/usage/rolling-upgrades-vpx/", 
            "text": "Rolling upgrades (VPX)\n\n\nThis document demonstrates how to to a rolling upgrade with zero\ndowntime for a simple load balanced service.\n\n\nThe methods showcased here are also applicable to more complex\nnetworking setups.\n\n\nSetup\n\n\nThe example utilizes Netscaler VPX and some virtualized hosts to provide\nthe back end web services.\n\n\nThe ansible playbooks along with other files needed to run this example\ncan be found at the following \ngithub\nrepository\n\n\nThe testbed required to run the examples is the following.\n\n\n    +--------------------+\n    | Netscaler VPX      |\n    |                    | 192.168.10.2                  +----------+\n    |               SNIP |\n-----------------------------\n| server 1 |\n    |                    |              |  192.168.10.10 +----------+\n    | NSIP          VIP  |              |                          ^\n    +--------------------+              |                          |10.78.60.204\n      ^              ^                  |            +----------+  |\n      |10.78.60.202  |10.78.60.203      +-----------\n| server 2 |  |\n      |              |                 192.168.10.11 +----------+  |\n      |              |                                       ^     |\n      |              |                           10.78.60.205|     |\n      |              |                                       |     |\n      |              |                                       |     |\n      |              |                                       |     |\n      |              |                                       |     |\n      |=NITRO        |=HTTP                                  |     |\n      |              |                                       |     |\n      |              |     +-----------+             SSH     |     |\n      +--------------+-----| user host |---------------------+-----+\n                           +-----------+\n\n\n\n\n\nWe need a virtual host to run Netscaler VPX and we also need two hosts\nto run the back end web services. These can be any kind of hosts, as\nlong as it is possible for the Netscaler node and the web server nodes\nto communicate via a specified subnet. Having the backend servers as\nvirtual hosts on the same Xen Server as the Netscaler VPX is recommended\nsince it simplifies the networking setup needed.\n\n\nIn our example the back end servers and the Netscaler host communicate\nvia the \n192.168.10.0/24\n subnet.\n\n\nAlso there is a user host which is the machine that will run the\nplaybooks for this example. This host needs to be able to communicate\nvia SSH with the back end servers to be able to setup and update the web\nservices and also needs to be able to make NITRO API calls to the\nNetscaler node on the configured NSIP.\n\n\nFinally Netscaler needs to have a Virtual IP configured which will be\nthe client facing address of our load balanced service.\n\n\n\n\nTip\n\n\n\n\n\"Note\n        The playbooks and scripts do not configure any of these ip addresses on the Netscaler node or the server nodes. You need to set them up prior to running the playbooks in this example and modify the \ninventory.txt\n file to match your particular configuration.\n\n\nMore details for the requirements of each node are included in the\nREADME file of the \ngithub\nrepository\n\ncontaining this example's files.\n\n\nInitializing the testbed\n\n\nHaving setup the testbed and modified the inventory.txt file to match\nthe configured ip addresses we need to initialize the Netscaler and the\nback end server nodes.\n\n\nThis is done by running on the user host from a fresh checkout of the\nfiles from the \ngithub\nrepository\n\nby running the following command\n\n\nansible-playbook -i inventory.txt rolling_init.yaml\n\n\n\n\n\nRunning this playbook will initialize the back end services and also\nconfigure the Netscaler in order to serve them over the VIP of the load\nbalancer.\n\n\nThe logical configuration of the Netscaler node can be seen in the\nfollowing diagram.\n\n\n          +\n          |\n          |\n          |\n+---------V----------+\n| Load balancer      |\n| lb_vserver_1       |----------+\n| 10.78.60.203:80    |          |\n+--------------------+          |\n        |                       |\n        |                       |\n        |                       |\n+-------V-----+           +-----V-------+\n| Service 1   |           | Service 2   |\n| server_1    |           | server_2    |\n| port 80     |           | port 80     |\n+-------------+           +-------------+\n        |                       |\n        |                       |\n        |                       |\n+-------V-------+           +-----V---------+\n| server_1      |           | server_2      |\n| 192.168.10.10 |           | 192.168.10.11 |\n+---------------+           +---------------+\n\n\n\n\n\nIn this setup the load balancer virtual server is configured with the\nROUNDROBIN load balancing method and has 2 service members with 50%\nweight each.\n\n\nTo check that the load balancer works correctly run the following\ncommand\n\n\ncurl \n10\n.78.60.203\n\n\n\n\n\nYou should see a \nHello webserver1\n. Running the same command a second\ntime should output \nHello webserver2\n.\n\n\nUpgrade process\n\n\nThe upgrade playbook utilizes the \npre_tasks\n and \npost_tasks\n hooks to\nbring the services down and back up during the update process.\n\n\nThe upgrade playbook is the following:\n\n\n-\n \nhosts\n:\n \nservice_hosts\n\n  \nvars\n:\n\n    \ncompose_yaml\n:\n \n/var/tmp/docker-compose.yaml\n\n\n  \nremote_user\n:\n \nroot\n\n  \ngather_facts\n:\n \nFalse\n\n  \nserial\n:\n \n1\n\n\n  \npre_tasks\n:\n\n    \n-\n \nname\n:\n \nDisable\n \n{{\n \nservername\n \n}}\n\n      \ndelegate_to\n:\n \nlocalhost\n\n      \nnetscaler_server\n:\n\n        \nnsip\n:\n \n{{\n \nnsip\n \n}}\n\n        \nnitro_user\n:\n \n{{\n \nnitro_user\n \n}}\n\n        \nnitro_pass\n:\n \n{{\n \nnitro_pass\n \n}}\n\n\n        \ndisabled\n:\n \nyes\n\n\n        \nname\n:\n \n{{\n \nservername\n \n}}\n\n\n  \npost_tasks\n:\n\n\n    \n-\n \nname\n:\n \nRe\n \nenable\n \n{{\n \nservername\n \n}}\n\n      \ndelegate_to\n:\n \nlocalhost\n\n      \nnetscaler_server\n:\n\n        \nnsip\n:\n \n{{\n \nnsip\n \n}}\n\n        \nnitro_user\n:\n \n{{\n \nnitro_user\n \n}}\n\n        \nnitro_pass\n:\n \n{{\n \nnitro_pass\n \n}}\n\n\n        \ndisabled\n:\n \nno\n\n        \nname\n:\n \n{{\n \nservername\n \n}}\n\n\n  \ntasks\n:\n\n\n    \n-\n \nname\n:\n \nUpdate\n \nbackend\n \n{{\n \nservername\n \n}}\n\n      \ncommand\n:\n \ndocker-compose -f \n{{ compose_yaml }}\n exec -d webserver bash -c \necho \nhello updated {{ servername }}\n \n /app/content.txt\n\n\n\n\n\n\nThe function of the pre_tasks and post_tasks hooks is documented by\n\nansible\n.\n\n\nEssentially what we do is that we disable the server entity in Netscaler\nfor each web service before the update process and after the update has\ntaken place we re enable the server entity.\n\n\nThe \nserial: 1\n option instructs ansible to operate on the webservers\none at a time. This is a deviation from the default behavior of Ansible\nwhich is to operate on multiple nodes at once.\n\n\nIn our example the update process is just a simple change of the content\nfile on the web service docker container to verify the update has taken\neffect.\n\n\nTo see how the update works you can run\n\n\ncurl \n10\n.78.60.203\n\n\n\n\n\nduring the update process and see how the output changes.\n\n\nSince the update itself is a relatively quick process you may not be\nable to see the rolling nature of the upgrade.\n\n\nFor that you may want to run the update script in step mode\n\n\nansible-playbook -i inventory.txt rolling_update.yml --step\n\n\n\n\n\nand watch the output of\n\n\ncurl \n10\n.78.60.203\n\n\n\n\n\na number of times to actually see what happens.\n\n\nWhat you should see is each server taken out of the load balancing pool\nand then brought up without any service interruption.\n\n\nIn our example the update of the web server is instantaneous we do not\nhave any actual down time.\n\n\nIn a real world situation the update would put the webserver in a state\nthat would be unable to respond to requests.\n\n\nHad we not disabled the corresponding server, in this case, would mean\nthat a number of requests would be directed to the offline server\nresulting in clients getting error responses.\n\n\nEventually the monitors attached to the Netscaler services would take\nthe disrupted service out of the load balancing pool but depending on\nthe traffic volume several requests would have been affected by the non\nfunctioning service by that time.\n\n\nDisabling the server before the update process guarantees that Netscaler\nwill not direct any traffic to it during that time, ensuring continuous\ndelivery of the content.\n\n\nReferences\n\n\n\n\nNetscaler ansible modules repository\n\n\nAnsible documentation", 
            "title": "Rolling upgrades (VPX)"
        }, 
        {
            "location": "/usage/rolling-upgrades-vpx/#rolling-upgrades-vpx", 
            "text": "This document demonstrates how to to a rolling upgrade with zero\ndowntime for a simple load balanced service.  The methods showcased here are also applicable to more complex\nnetworking setups.", 
            "title": "Rolling upgrades (VPX)"
        }, 
        {
            "location": "/usage/rolling-upgrades-vpx/#setup", 
            "text": "The example utilizes Netscaler VPX and some virtualized hosts to provide\nthe back end web services.  The ansible playbooks along with other files needed to run this example\ncan be found at the following  github\nrepository  The testbed required to run the examples is the following.      +--------------------+\n    | Netscaler VPX      |\n    |                    | 192.168.10.2                  +----------+\n    |               SNIP | ----------------------------- | server 1 |\n    |                    |              |  192.168.10.10 +----------+\n    | NSIP          VIP  |              |                          ^\n    +--------------------+              |                          |10.78.60.204\n      ^              ^                  |            +----------+  |\n      |10.78.60.202  |10.78.60.203      +----------- | server 2 |  |\n      |              |                 192.168.10.11 +----------+  |\n      |              |                                       ^     |\n      |              |                           10.78.60.205|     |\n      |              |                                       |     |\n      |              |                                       |     |\n      |              |                                       |     |\n      |              |                                       |     |\n      |=NITRO        |=HTTP                                  |     |\n      |              |                                       |     |\n      |              |     +-----------+             SSH     |     |\n      +--------------+-----| user host |---------------------+-----+\n                           +-----------+  We need a virtual host to run Netscaler VPX and we also need two hosts\nto run the back end web services. These can be any kind of hosts, as\nlong as it is possible for the Netscaler node and the web server nodes\nto communicate via a specified subnet. Having the backend servers as\nvirtual hosts on the same Xen Server as the Netscaler VPX is recommended\nsince it simplifies the networking setup needed.  In our example the back end servers and the Netscaler host communicate\nvia the  192.168.10.0/24  subnet.  Also there is a user host which is the machine that will run the\nplaybooks for this example. This host needs to be able to communicate\nvia SSH with the back end servers to be able to setup and update the web\nservices and also needs to be able to make NITRO API calls to the\nNetscaler node on the configured NSIP.  Finally Netscaler needs to have a Virtual IP configured which will be\nthe client facing address of our load balanced service.   Tip   \"Note\n        The playbooks and scripts do not configure any of these ip addresses on the Netscaler node or the server nodes. You need to set them up prior to running the playbooks in this example and modify the  inventory.txt  file to match your particular configuration.  More details for the requirements of each node are included in the\nREADME file of the  github\nrepository \ncontaining this example's files.", 
            "title": "Setup"
        }, 
        {
            "location": "/usage/rolling-upgrades-vpx/#initializing-the-testbed", 
            "text": "Having setup the testbed and modified the inventory.txt file to match\nthe configured ip addresses we need to initialize the Netscaler and the\nback end server nodes.  This is done by running on the user host from a fresh checkout of the\nfiles from the  github\nrepository \nby running the following command  ansible-playbook -i inventory.txt rolling_init.yaml  Running this playbook will initialize the back end services and also\nconfigure the Netscaler in order to serve them over the VIP of the load\nbalancer.  The logical configuration of the Netscaler node can be seen in the\nfollowing diagram.            +\n          |\n          |\n          |\n+---------V----------+\n| Load balancer      |\n| lb_vserver_1       |----------+\n| 10.78.60.203:80    |          |\n+--------------------+          |\n        |                       |\n        |                       |\n        |                       |\n+-------V-----+           +-----V-------+\n| Service 1   |           | Service 2   |\n| server_1    |           | server_2    |\n| port 80     |           | port 80     |\n+-------------+           +-------------+\n        |                       |\n        |                       |\n        |                       |\n+-------V-------+           +-----V---------+\n| server_1      |           | server_2      |\n| 192.168.10.10 |           | 192.168.10.11 |\n+---------------+           +---------------+  In this setup the load balancer virtual server is configured with the\nROUNDROBIN load balancing method and has 2 service members with 50%\nweight each.  To check that the load balancer works correctly run the following\ncommand  curl  10 .78.60.203  You should see a  Hello webserver1 . Running the same command a second\ntime should output  Hello webserver2 .", 
            "title": "Initializing the testbed"
        }, 
        {
            "location": "/usage/rolling-upgrades-vpx/#upgrade-process", 
            "text": "The upgrade playbook utilizes the  pre_tasks  and  post_tasks  hooks to\nbring the services down and back up during the update process.  The upgrade playbook is the following:  -   hosts :   service_hosts \n   vars : \n     compose_yaml :   /var/tmp/docker-compose.yaml \n\n   remote_user :   root \n   gather_facts :   False \n   serial :   1 \n\n   pre_tasks : \n     -   name :   Disable   {{   servername   }} \n       delegate_to :   localhost \n       netscaler_server : \n         nsip :   {{   nsip   }} \n         nitro_user :   {{   nitro_user   }} \n         nitro_pass :   {{   nitro_pass   }} \n\n         disabled :   yes \n\n         name :   {{   servername   }} \n\n   post_tasks : \n\n     -   name :   Re   enable   {{   servername   }} \n       delegate_to :   localhost \n       netscaler_server : \n         nsip :   {{   nsip   }} \n         nitro_user :   {{   nitro_user   }} \n         nitro_pass :   {{   nitro_pass   }} \n\n         disabled :   no \n         name :   {{   servername   }} \n\n   tasks : \n\n     -   name :   Update   backend   {{   servername   }} \n       command :   docker-compose -f  {{ compose_yaml }}  exec -d webserver bash -c  echo  hello updated {{ servername }}    /app/content.txt   The function of the pre_tasks and post_tasks hooks is documented by ansible .  Essentially what we do is that we disable the server entity in Netscaler\nfor each web service before the update process and after the update has\ntaken place we re enable the server entity.  The  serial: 1  option instructs ansible to operate on the webservers\none at a time. This is a deviation from the default behavior of Ansible\nwhich is to operate on multiple nodes at once.  In our example the update process is just a simple change of the content\nfile on the web service docker container to verify the update has taken\neffect.  To see how the update works you can run  curl  10 .78.60.203  during the update process and see how the output changes.  Since the update itself is a relatively quick process you may not be\nable to see the rolling nature of the upgrade.  For that you may want to run the update script in step mode  ansible-playbook -i inventory.txt rolling_update.yml --step  and watch the output of  curl  10 .78.60.203  a number of times to actually see what happens.  What you should see is each server taken out of the load balancing pool\nand then brought up without any service interruption.  In our example the update of the web server is instantaneous we do not\nhave any actual down time.  In a real world situation the update would put the webserver in a state\nthat would be unable to respond to requests.  Had we not disabled the corresponding server, in this case, would mean\nthat a number of requests would be directed to the offline server\nresulting in clients getting error responses.  Eventually the monitors attached to the Netscaler services would take\nthe disrupted service out of the load balancing pool but depending on\nthe traffic volume several requests would have been affected by the non\nfunctioning service by that time.  Disabling the server before the update process guarantees that Netscaler\nwill not direct any traffic to it during that time, ensuring continuous\ndelivery of the content.", 
            "title": "Upgrade process"
        }, 
        {
            "location": "/usage/rolling-upgrades-vpx/#references", 
            "text": "Netscaler ansible modules repository  Ansible documentation", 
            "title": "References"
        }, 
        {
            "location": "/usage/docker-image/", 
            "text": "Netscaler ansible docker image\n\n\nTo make the running of netscaler ansible modules easier a ready\nto run image exists which does not require\nany software packages installed on the host other than the docker engine.\n\n\nIt is suitable for running quickly and easily simple playbooks that are using\nansible modules and core ansible modules.\n\n\nThe image is not suitable for running any arbitraty ansible playbook since\nmany non core modules require extra dependencies which this docker\nimage does not have installed.\n\n\nInstallation\n\n\nThe installation is quite simple.\n\n\ndocker pull giorgosnikolopoulos/netscaler-ansible:latest\n\n\n\n\n\nUsage\n\n\nThe entrypoint of the docker image is the \nansible-playbook\n command.\n\n\nThis means that it can be used as drop in replacement of this command.\n\n\nRunning\n\n\ndocker run --rm giorgosnikolopoulos/netscaler-ansible:latest\n\n\n\n\n\nWill output the help for the command \nansible-playbook\n\n\nTo run a playbook we need to map a directory of the host to a directory\nof the docker container so that the inventory and playbook files are\naccessible from inside the docker container.\n\n\nSo provided we have in the current directory an inventory file and a playbook\nby running the following\n\n\ndocker run --rm -v $(pwd):/pwd giorgosnikolopoulos/netscaler-ansible -i inventory playbook.yml\n\n\n\n\n\nThe playbook will be executed.\n\n\nOf course the container needs to have also access to the NetScaler\nnode being configured otherwise the execution will fail.\n\n\nExample\n\n\nFollowing is an example of how to use the container along\nwith a NetScaler CPX deployment.\n\n\nWe will use \ndocker-compose\n to setup our testbed using the following\ncompose \ndocker-compose.yaml\n file.\n\n\nversion: \n2\n\n\nservices:\n    cpx:\n        image: giorgosnikolopoulos/cpx:12.0-41.22\n       ports:\n         - \n22\n\n         - \n80\n\n         - \n443\n\n         - \n161\n\n       environment:\n         EULA: \nyes\n\n       ulimits:\n         core: -1\n       tty: true\n       stdin_open: true\n       privileged: true\n\n   netscaler-ansible:\n       image: giorgosnikolopoulos/netscaler-ansible:latest\n       tty: true\n       stdin_open: true\n       volumes:\n         - .:/pwd\n       links:\n         - cpx\n\n\n\n\n\nNote that in addition to instantiating the cpx and netscaler-ansible\ncontainers we also map the current directory to the working directory\nof the netscaler-ansible container and we also create a link to the cpx\ncontainer.\n\n\nThis has the effect that the host's current directory is exposed as it is\ninside the docker container and that there is a network reference to the\ncpx container which can be used instead of the ip address of the container.\n\n\nFor our example we will use the following \ninventory.txt\n file.\n\n\n[netscaler]\n\n\n\nnetscaler_cpx nsip\n=\ncpx nitro_user=nsroot nitro_pass=nsroot\n\n\n\n\n\n\nFor our sample playbook we will use the following \nplay.yaml\n\n\n---\n\n\n\n- hosts: netscaler\n\n\n  gather_facts: false\n\n\n\n  tasks:\n\n\n    - name: lb vserver\n\n\n      delegate_to: localhost\n\n\n      netscaler_lb_vserver:\n\n\n        nsip: \n{{\n \nnsip\n \n}}\n\n\n        nitro_user: \n{{\n \nnitro_user\n \n}}\n\n\n        nitro_pass: \n{{\n \nnitro_pass\n \n}}\n\n\n\n\n        name: lb-vserver-1\n\n\n        servicetype: HTTP\n\n\n        ipv46: 6.92.2.2\n\n\n        port: 80\n\n\n\n    - name: cs action\n\n\n        delegate_to: localhost\n\n\n        netscaler_cs_action:\n\n\n          nsip: \n{{\n \nnsip\n \n}}\n\n\n          nitro_user: \n{{\n \nnitro_user\n \n}}\n\n\n          nitro_pass: \n{{\n \nnitro_pass\n \n}}\n\n\n\n          name: action1\n\n\n          targetlbvserver: lb-vserver-1\n\n\n\n\n\n\nThese files are located in the same directory as the \ndocker-compose.yaml\n\nfile.\n\n\nFirst we bring the containers up.\n\n\ndocker-compose up -d\n\n\n\n\n\nVerify that the containers are setup by running\n\n\ndocker-compose ps\n\n\n\n\n\nYou should see that the cpx container is up and running\nand that the netscaler-ansible container has exited.\n\n\nFrom this point on we can use the \ndocker-compose run netscaler-ansible\n command\nto run our playbooks.\n\n\nTo run the sample playbook run:\n\n\ndocker-compose run netscaler-ansible -i inventory.txt play.yaml\n\n\n\n\n\nYou should see the output of the playbook run just as if you had\nrun \nansible-playbook\n normally.\n\n\nAny valid \nansible-playbook\n option can be passed on the command line to\nthe \nnetscaler-ansible\n container.\n\n\nWhen you no longer need the testbed you can tear it down by running:\n\n\ndocker-compose stop\ndocker-compose rm", 
            "title": "Netscaler ansible docker image"
        }, 
        {
            "location": "/usage/docker-image/#netscaler-ansible-docker-image", 
            "text": "To make the running of netscaler ansible modules easier a ready\nto run image exists which does not require\nany software packages installed on the host other than the docker engine.  It is suitable for running quickly and easily simple playbooks that are using\nansible modules and core ansible modules.  The image is not suitable for running any arbitraty ansible playbook since\nmany non core modules require extra dependencies which this docker\nimage does not have installed.", 
            "title": "Netscaler ansible docker image"
        }, 
        {
            "location": "/usage/docker-image/#installation", 
            "text": "The installation is quite simple.  docker pull giorgosnikolopoulos/netscaler-ansible:latest", 
            "title": "Installation"
        }, 
        {
            "location": "/usage/docker-image/#usage", 
            "text": "The entrypoint of the docker image is the  ansible-playbook  command.  This means that it can be used as drop in replacement of this command.  Running  docker run --rm giorgosnikolopoulos/netscaler-ansible:latest  Will output the help for the command  ansible-playbook  To run a playbook we need to map a directory of the host to a directory\nof the docker container so that the inventory and playbook files are\naccessible from inside the docker container.  So provided we have in the current directory an inventory file and a playbook\nby running the following  docker run --rm -v $(pwd):/pwd giorgosnikolopoulos/netscaler-ansible -i inventory playbook.yml  The playbook will be executed.  Of course the container needs to have also access to the NetScaler\nnode being configured otherwise the execution will fail.", 
            "title": "Usage"
        }, 
        {
            "location": "/usage/docker-image/#example", 
            "text": "Following is an example of how to use the container along\nwith a NetScaler CPX deployment.  We will use  docker-compose  to setup our testbed using the following\ncompose  docker-compose.yaml  file.  version:  2 \n\nservices:\n    cpx:\n        image: giorgosnikolopoulos/cpx:12.0-41.22\n       ports:\n         -  22 \n         -  80 \n         -  443 \n         -  161 \n       environment:\n         EULA:  yes \n       ulimits:\n         core: -1\n       tty: true\n       stdin_open: true\n       privileged: true\n\n   netscaler-ansible:\n       image: giorgosnikolopoulos/netscaler-ansible:latest\n       tty: true\n       stdin_open: true\n       volumes:\n         - .:/pwd\n       links:\n         - cpx  Note that in addition to instantiating the cpx and netscaler-ansible\ncontainers we also map the current directory to the working directory\nof the netscaler-ansible container and we also create a link to the cpx\ncontainer.  This has the effect that the host's current directory is exposed as it is\ninside the docker container and that there is a network reference to the\ncpx container which can be used instead of the ip address of the container.  For our example we will use the following  inventory.txt  file.  [netscaler]  netscaler_cpx nsip = cpx nitro_user=nsroot nitro_pass=nsroot   For our sample playbook we will use the following  play.yaml  ---  - hosts: netscaler    gather_facts: false    tasks:      - name: lb vserver        delegate_to: localhost        netscaler_lb_vserver:          nsip:  {{   nsip   }}          nitro_user:  {{   nitro_user   }}          nitro_pass:  {{   nitro_pass   }}          name: lb-vserver-1          servicetype: HTTP          ipv46: 6.92.2.2          port: 80      - name: cs action          delegate_to: localhost          netscaler_cs_action:            nsip:  {{   nsip   }}            nitro_user:  {{   nitro_user   }}            nitro_pass:  {{   nitro_pass   }}            name: action1            targetlbvserver: lb-vserver-1   These files are located in the same directory as the  docker-compose.yaml \nfile.  First we bring the containers up.  docker-compose up -d  Verify that the containers are setup by running  docker-compose ps  You should see that the cpx container is up and running\nand that the netscaler-ansible container has exited.  From this point on we can use the  docker-compose run netscaler-ansible  command\nto run our playbooks.  To run the sample playbook run:  docker-compose run netscaler-ansible -i inventory.txt play.yaml  You should see the output of the playbook run just as if you had\nrun  ansible-playbook  normally.  Any valid  ansible-playbook  option can be passed on the command line to\nthe  netscaler-ansible  container.  When you no longer need the testbed you can tear it down by running:  docker-compose stop\ndocker-compose rm", 
            "title": "Example"
        }, 
        {
            "location": "/generic-modules/about/", 
            "text": "Using generic Ansible modules\n\n\nThe Netscaler Ansible modules try to accomodate the most frequently\nchanging items of the Netscaler configuration. Things that change from\nday to day operations.\n\n\nIn this section we investigate how to leverage Ansible standard modules\nto configure Netscaler to cover the cases where the user needs to use\nAnsible for a Netscaler configuration entity that does not have a\nspecialized Ansible module.\n\n\nWe make use of the Ansible uri module mainly to issue NITRO API requests\nto Netscaler.\n\n\nThe solutions we present here do have drawbacks compared to the use of\nspecialized Netscaler Ansbile modules, such as not having a check mode\noperation, having to check for NITRO errors and handle them accordingly,\nand also having to account for particularities that a configuration\nentity may have.\n\n\nAll these issues are taken care of in the Netscaler specific modules but\nin the solutions we present here the user has to deal with all of these.\n\n\nThe source files referenced in the following sections along with more\nexamples can be found on this \ngithub repository\n.\n\n\nReferences\n\n\n\n\nNITRO API overview\n\n\nNITRO API reference", 
            "title": "Using generic Ansible modules"
        }, 
        {
            "location": "/generic-modules/about/#using-generic-ansible-modules", 
            "text": "The Netscaler Ansible modules try to accomodate the most frequently\nchanging items of the Netscaler configuration. Things that change from\nday to day operations.  In this section we investigate how to leverage Ansible standard modules\nto configure Netscaler to cover the cases where the user needs to use\nAnsible for a Netscaler configuration entity that does not have a\nspecialized Ansible module.  We make use of the Ansible uri module mainly to issue NITRO API requests\nto Netscaler.  The solutions we present here do have drawbacks compared to the use of\nspecialized Netscaler Ansbile modules, such as not having a check mode\noperation, having to check for NITRO errors and handle them accordingly,\nand also having to account for particularities that a configuration\nentity may have.  All these issues are taken care of in the Netscaler specific modules but\nin the solutions we present here the user has to deal with all of these.  The source files referenced in the following sections along with more\nexamples can be found on this  github repository .", 
            "title": "Using generic Ansible modules"
        }, 
        {
            "location": "/generic-modules/about/#references", 
            "text": "NITRO API overview  NITRO API reference", 
            "title": "References"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/", 
            "text": "Templating the configuration file\n\n\nOne method of configuring Netscaler consists of editing the ns.conf file\ndirectly and then rebooting Netscaler for the configuration changes to\ntake effect.\n\n\nAfter the reboot the saved configuration becomes the running\nconfiguration which is what we want to change.\n\n\nWorkflow\n\n\nWith this method we leverage Ansible's \ntemplate module\n to\nproduce a ns.conf file from a Jinja2 template.\n\n\nThe Jinja2 template is populated from configuration variables which can\nbe defined with various methods, inside the playbook, in an inventory\nfile or loaded from inventory files.\n\n\nWe then upload the resulting ns.conf to the Netscaler node which alters\nthe saved configuration.\n\n\nFor the saved configuration to become running configuration we need to\nreboot Netscaler. Doing a warm reboot is recommended since it is\nsufficient to reload the configuration and also avoid the greater\ndowntime a cold reboot would induce.\n\n\nAfter the reboot the user can check the running configuration either\nthrough the GUI or the command line interface and make sure the changes\nhave been succesfully applied.\n\n\nThere is an assortment of playbooks on this \ngithub repository\n which\ncontains sample playbooks that perform fundamental NITRO operations. The\ntasks within each playbook can be combined into a larger playbook which\naccomplishes a full Netscaler configuration.\n\n\nIn fact this is how the following example was constructed.\n\n\nPlaybook\n\n\nIn the following example we showcase how we can setup a load balancer\nwhich balances two backend services. The full content of the referenced\nfiles can be found \nhere\n.\n\n\nProcessing the template\n\n\nFirst we have a Jinja template file to produce the desired ns.conf. It\nis recommended to use an actual ns.conf file from the target Netscaler\nnode as a starting point for the template.\n\n\nThe full file is quite long but the interesting parts are shown below.\n\n\n...\n\n\n\n# Start of Jinja inserted servers\n\n\n{\n%\n \nfor server in configuration.servers %\n}\n\n\nadd server {{ server.name }} {{ server.ipaddress }}\n\n\n{% endfor %}\n\n\n# End of Jinja inserted servers\n\n\n\n...\n\n\n\n# Start of Jinja inserted services\n\n\n{\n%\n \nfor service in configuration.services %\n}\n\n\nadd service {{ service.name }} {{ service.ipaddress }} {{ service.type }} {{ service.port }} -gslb NONE -maxClient 0 -maxReq 0 -cip DISABLED -usip NO -useproxyport YES -sp OFF -cltTimeout 180 -svrTimeout 360 -CKA NO -TCPB NO -CMP NO\n\n\n{% endfor %}\n\n\n# End of Jinja inserted services\n\n\n\n\n...\n\n\n\n# Start of Jinja inserted lb vservers\n\n\n{\n%\n \nfor vserver in configuration.lbvservers %\n}\n\n\nadd lb vserver {{ vserver.name }} {{ vserver.type }} {{ vserver.ipaddress }} {{ vserver.port }} -persistenceType NONE -cltTimeout 180\n\n\n{% endfor %}\n\n\n# End of Jinja inserted lb vservers\n\n\n\n...\n\n\n\n# Start of Jinja inserted lb vservers binds\n\n\n{\n%\n \nfor bind in configuration.lbvserver_binds %\n}\n\n\nbind lb vserver {{ bind.server }} {{ bind.service }} -weight {{ bind.weight }}\n\n\n{% endfor %}\n\n\n# End of Jinja inserted lb vservers binds\n\n\n\n\n\n\nEssentially we iterate over items in the configuration dictionary. This\ndictionary is populated from the playbook variables.\n\n\nDefining the configuration variables\n\n\nThe playbook variables are shown below.\n\n\nvars\n:\n\n  \nfilename\n:\n \nns.conf\n\n  \nfilelocation\n:\n \n/nsconfig\n\n  \nlocalfile\n:\n \n/var/tmp/ns.conf\n\n\n  \nwarm_reboot\n:\n \nyes\n\n\n  \nconfiguration\n:\n\n    \nservers\n:\n\n      \n-\n \nname\n:\n \n192.168.1.1\n\n        \nipaddress\n:\n \n192.168.1.1\n\n      \n-\n \nname\n:\n \n192.168.1.2\n\n        \nipaddress\n:\n \n192.168.1.2\n\n\n    \nservices\n:\n\n      \n-\n \nname\n:\n \nservice-test-1\n\n        \nipaddress\n:\n \n192.168.1.1\n\n        \nport\n:\n \n80\n\n        \ntype\n:\n \nHTTP\n\n\n      \n-\n \nname\n:\n \nservice-test-2\n\n        \nipaddress\n:\n \n192.168.1.2\n\n        \nport\n:\n \n80\n\n        \ntype\n:\n \nHTTP\n\n\n    \nlbvservers\n:\n\n      \n-\n \nname\n:\n \nserver-test\n\n        \nipaddress\n:\n \n10.78.60.203\n\n        \nport\n:\n \n80\n\n        \ntype\n:\n \nHTTP\n\n\n    \nlbvserver_binds\n:\n\n      \n-\n \nserver\n:\n \nserver-test\n\n        \nservice\n:\n \nservice-test-1\n\n        \nweight\n:\n \n50\n\n      \n-\n \nserver\n:\n \nserver-test\n\n        \nservice\n:\n \nservice-test-2\n\n        \nweight\n:\n \n50\n\n\n\n\n\n\nThe configuration dictionary is defined inside the playbook. This is\ndone for maintaining simplicity in the context of the example.\n\n\nA more sophisticated setup could have defined the configuration\ndictionary in a separate variables file, in the inventory file or use\nany other method Ansible allows to define variables.\n\n\nWe also see the variables that configure the paths of the source and\ntarget files. These could also be defined in the different ways the\nconfiguration dictionary is defined.\n\n\nUpload the new ns.conf\n\n\nHaving produced the ns.conf file we need to upload it to Netscaler.\n\n\nFollowing are the tasks that accomplish this.\n\n\n-\n \nname\n:\n \nDelete old ns.conf\n\n  \ndelegate_to\n:\n \nlocalhost\n\n  \nuri\n:\n\n    \nurl\n:\n \nhttp://{{\n \nnsip\n \n}}/nitro/v1/config/systemfile?args=filename:{{\n \nfilename\n \n}},filelocation:{{\n \nfilelocation\n \n|\n \nreplace(\n/\n,\n%2F\n)\n \n}}\n\n    \nmethod\n:\n \nDELETE\n\n    \nstatus_code\n:\n \n200\n\n    \nreturn_content\n:\n \nyes\n\n    \nheaders\n:\n\n      \nX-NITRO-USER\n:\n \n{{\n \nnitro_user\n \n}}\n\n      \nX-NITRO-PASS\n:\n \n{{\n \nnitro_pass\n \n}}\n\n\n\n-\n \nname\n:\n \nUpload new ns.conf\n\n  \ndelegate_to\n:\n \nlocalhost\n\n  \nuri\n:\n\n    \nurl\n:\n \nhttp://{{\n \nnsip\n \n}}/nitro/v1/config/systemfile\n\n    \nmethod\n:\n \nPOST\n\n    \nstatus_code\n:\n \n201\n\n    \nreturn_content\n:\n \nyes\n\n    \nheaders\n:\n\n      \nX-NITRO-USER\n:\n \n{{\n \nnitro_user\n \n}}\n\n      \nX-NITRO-PASS\n:\n \n{{\n \nnitro_pass\n \n}}\n\n    \nbody_format\n:\n \njson\n\n    \nbody\n:\n\n      \nsystemfile\n:\n\n        \nfilename\n:\n \n{{\n \nfilename\n \n}}\n\n        \nfilecontent\n:\n \n{{\n \nlookup(\nfile\n,\n \nlocalfile)\n \n|\n \nb64encode\n \n}}\n\n        \nfilelocation\n:\n \n{{\n \nfilelocation\n \n}}\n\n\n\n\n\n\nNotice that we need to delete the existing file before copying the new\none. Trying to upload a file to an existing file path will result in a\nNITRO error.\n\n\nRebooting Netscaler\n\n\nThe last step is to warm reboot the Netscaler node. Replacing the\nns.conf file overwrites the saved configuration. The running\nconfiguration of Netscaler remains unaffected. To force Netscaler to\napply the saved configuration we need to reboot it. We have the option\ndo a warm reboot which results in less downtime than a full reboot.\n\n\nThe task that accomplishes this is shown below.\n\n\n-\n \nname\n:\n \nReboot Netscaler\n\n  \ndelegate_to\n:\n \nlocalhost\n\n  \nuri\n:\n\n    \nurl\n:\n \nhttp://{{\n \nnsip\n \n}}/nitro/v1/config/reboot\n\n    \nmethod\n:\n \nPOST\n\n    \nstatus_code\n:\n \n201\n\n    \nheaders\n:\n\n      \nX-NITRO-USER\n:\n \n{{\n \nnitro_user\n \n}}\n\n      \nX-NITRO-PASS\n:\n \n{{\n \nnitro_pass\n \n}}\n\n    \nbody_format\n:\n \njson\n\n    \nbody\n:\n\n      \nreboot\n:\n\n        \nwarm\n:\n \n{{\n \nwarm_reboot\n \n}}\n\n\n\n\n\n\nFinal points\n\n\nThe user needs for this example to set the variables needed for\nauthentication and communication with Netscaler. Namely \nnsip\n,\n\nnitro_user\n, \nnitro_pass\n. These variables retain the meaning they have\nin the Netscaler specific Ansible modules.\n\n\nAll tasks are run with the \ndelegate_to: localhost\n option set. This is\nneeded since we are making NITRO API calls to the Netscaler node. We do\nnot want to connect directly with SSH to it.\n\n\nIn some deployments the delegated host may need to be the bastion node\nthat has actual NITRO access to the Netscaler node.\n\n\nReferences\n\n\n\n\nAnsible NITRO API calls repository\n\n\nAnsible template module documentation", 
            "title": "Templating the configuration file"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#templating-the-configuration-file", 
            "text": "One method of configuring Netscaler consists of editing the ns.conf file\ndirectly and then rebooting Netscaler for the configuration changes to\ntake effect.  After the reboot the saved configuration becomes the running\nconfiguration which is what we want to change.", 
            "title": "Templating the configuration file"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#workflow", 
            "text": "With this method we leverage Ansible's  template module  to\nproduce a ns.conf file from a Jinja2 template.  The Jinja2 template is populated from configuration variables which can\nbe defined with various methods, inside the playbook, in an inventory\nfile or loaded from inventory files.  We then upload the resulting ns.conf to the Netscaler node which alters\nthe saved configuration.  For the saved configuration to become running configuration we need to\nreboot Netscaler. Doing a warm reboot is recommended since it is\nsufficient to reload the configuration and also avoid the greater\ndowntime a cold reboot would induce.  After the reboot the user can check the running configuration either\nthrough the GUI or the command line interface and make sure the changes\nhave been succesfully applied.  There is an assortment of playbooks on this  github repository  which\ncontains sample playbooks that perform fundamental NITRO operations. The\ntasks within each playbook can be combined into a larger playbook which\naccomplishes a full Netscaler configuration.  In fact this is how the following example was constructed.", 
            "title": "Workflow"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#playbook", 
            "text": "In the following example we showcase how we can setup a load balancer\nwhich balances two backend services. The full content of the referenced\nfiles can be found  here .", 
            "title": "Playbook"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#processing-the-template", 
            "text": "First we have a Jinja template file to produce the desired ns.conf. It\nis recommended to use an actual ns.conf file from the target Netscaler\nnode as a starting point for the template.  The full file is quite long but the interesting parts are shown below.  ...  # Start of Jinja inserted servers  { %   for server in configuration.servers % }  add server {{ server.name }} {{ server.ipaddress }}  {% endfor %}  # End of Jinja inserted servers  ...  # Start of Jinja inserted services  { %   for service in configuration.services % }  add service {{ service.name }} {{ service.ipaddress }} {{ service.type }} {{ service.port }} -gslb NONE -maxClient 0 -maxReq 0 -cip DISABLED -usip NO -useproxyport YES -sp OFF -cltTimeout 180 -svrTimeout 360 -CKA NO -TCPB NO -CMP NO  {% endfor %}  # End of Jinja inserted services  ...  # Start of Jinja inserted lb vservers  { %   for vserver in configuration.lbvservers % }  add lb vserver {{ vserver.name }} {{ vserver.type }} {{ vserver.ipaddress }} {{ vserver.port }} -persistenceType NONE -cltTimeout 180  {% endfor %}  # End of Jinja inserted lb vservers  ...  # Start of Jinja inserted lb vservers binds  { %   for bind in configuration.lbvserver_binds % }  bind lb vserver {{ bind.server }} {{ bind.service }} -weight {{ bind.weight }}  {% endfor %}  # End of Jinja inserted lb vservers binds   Essentially we iterate over items in the configuration dictionary. This\ndictionary is populated from the playbook variables.", 
            "title": "Processing the template"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#defining-the-configuration-variables", 
            "text": "The playbook variables are shown below.  vars : \n   filename :   ns.conf \n   filelocation :   /nsconfig \n   localfile :   /var/tmp/ns.conf \n\n   warm_reboot :   yes \n\n   configuration : \n     servers : \n       -   name :   192.168.1.1 \n         ipaddress :   192.168.1.1 \n       -   name :   192.168.1.2 \n         ipaddress :   192.168.1.2 \n\n     services : \n       -   name :   service-test-1 \n         ipaddress :   192.168.1.1 \n         port :   80 \n         type :   HTTP \n\n       -   name :   service-test-2 \n         ipaddress :   192.168.1.2 \n         port :   80 \n         type :   HTTP \n\n     lbvservers : \n       -   name :   server-test \n         ipaddress :   10.78.60.203 \n         port :   80 \n         type :   HTTP \n\n     lbvserver_binds : \n       -   server :   server-test \n         service :   service-test-1 \n         weight :   50 \n       -   server :   server-test \n         service :   service-test-2 \n         weight :   50   The configuration dictionary is defined inside the playbook. This is\ndone for maintaining simplicity in the context of the example.  A more sophisticated setup could have defined the configuration\ndictionary in a separate variables file, in the inventory file or use\nany other method Ansible allows to define variables.  We also see the variables that configure the paths of the source and\ntarget files. These could also be defined in the different ways the\nconfiguration dictionary is defined.", 
            "title": "Defining the configuration variables"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#upload-the-new-nsconf", 
            "text": "Having produced the ns.conf file we need to upload it to Netscaler.  Following are the tasks that accomplish this.  -   name :   Delete old ns.conf \n   delegate_to :   localhost \n   uri : \n     url :   http://{{   nsip   }}/nitro/v1/config/systemfile?args=filename:{{   filename   }},filelocation:{{   filelocation   |   replace( / , %2F )   }} \n     method :   DELETE \n     status_code :   200 \n     return_content :   yes \n     headers : \n       X-NITRO-USER :   {{   nitro_user   }} \n       X-NITRO-PASS :   {{   nitro_pass   }}  -   name :   Upload new ns.conf \n   delegate_to :   localhost \n   uri : \n     url :   http://{{   nsip   }}/nitro/v1/config/systemfile \n     method :   POST \n     status_code :   201 \n     return_content :   yes \n     headers : \n       X-NITRO-USER :   {{   nitro_user   }} \n       X-NITRO-PASS :   {{   nitro_pass   }} \n     body_format :   json \n     body : \n       systemfile : \n         filename :   {{   filename   }} \n         filecontent :   {{   lookup( file ,   localfile)   |   b64encode   }} \n         filelocation :   {{   filelocation   }}   Notice that we need to delete the existing file before copying the new\none. Trying to upload a file to an existing file path will result in a\nNITRO error.", 
            "title": "Upload the new ns.conf"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#rebooting-netscaler", 
            "text": "The last step is to warm reboot the Netscaler node. Replacing the\nns.conf file overwrites the saved configuration. The running\nconfiguration of Netscaler remains unaffected. To force Netscaler to\napply the saved configuration we need to reboot it. We have the option\ndo a warm reboot which results in less downtime than a full reboot.  The task that accomplishes this is shown below.  -   name :   Reboot Netscaler \n   delegate_to :   localhost \n   uri : \n     url :   http://{{   nsip   }}/nitro/v1/config/reboot \n     method :   POST \n     status_code :   201 \n     headers : \n       X-NITRO-USER :   {{   nitro_user   }} \n       X-NITRO-PASS :   {{   nitro_pass   }} \n     body_format :   json \n     body : \n       reboot : \n         warm :   {{   warm_reboot   }}", 
            "title": "Rebooting Netscaler"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#final-points", 
            "text": "The user needs for this example to set the variables needed for\nauthentication and communication with Netscaler. Namely  nsip , nitro_user ,  nitro_pass . These variables retain the meaning they have\nin the Netscaler specific Ansible modules.  All tasks are run with the  delegate_to: localhost  option set. This is\nneeded since we are making NITRO API calls to the Netscaler node. We do\nnot want to connect directly with SSH to it.  In some deployments the delegated host may need to be the bastion node\nthat has actual NITRO access to the Netscaler node.", 
            "title": "Final points"
        }, 
        {
            "location": "/generic-modules/templating-configuration-file/#references", 
            "text": "Ansible NITRO API calls repository  Ansible template module documentation", 
            "title": "References"
        }, 
        {
            "location": "/generic-modules/nitro-api-calls/", 
            "text": "Direct NITRO API calls\n\n\nOne method of configuring Netscaler consists of making direct NITRO API\ncalls using Ansbile's \nuri module\n.\n\n\nThis method tends to be quite verbose since setting up even basic\nfunctions requires multiple NITRO calls.\n\n\nAnother consideration is failure robustness. NITRO API call failures\nresult in the uri module task failing immediately and stopping the\nexecution of the rest of the playbook.\n\n\nThis may be desired behavior in general since in some cases we need to\nexamine the failure response to actually determine if the operation was\nindeed a failure or expected.\n\n\nAn example of that would be trying to add a resource while it exists.\nThis is will result in failure since the HTTP POST request will not\ncreate the resource but this does not mean the configuration of\nNetscaler is necessarily invalid.\n\n\nUsing Ansible's conditional constructs we can work around this problem\nin most cases but this adds to the verbosity and complexity of the\nplaybooks.\n\n\nWorkflow\n\n\nIn the following example we use direct NITRO API calls to create or\nupdate a basic server.\n\n\nThe play would be quite short but we have added some control logic to\ndetect whether the resource already exists and then apply the\nappropriate operation.\n\n\nWe first try to get the details of the configuration resource. We\nexamine the outcome of this operation and if it was successful we\nproceed to update the resource. If it failed we examine the exact\nerrorcode and if it signifies that the error was due to the resource\nmissing we proceed to create it.\n\n\nOn any other outcome, an error that was not what was expected, the play\nfails.\n\n\nThe final task is to save the running configuration to ensure that a\nreboot of Netscaler will not undo the changes we have made.\n\n\nPlaybook\n\n\n-\n \nhosts\n:\n \nnetscaler\n\n  \ngather_facts\n:\n \nno\n\n  \nvars\n:\n\n    \nresource\n:\n \nserver\n\n    \nrequest_payload\n:\n\n      \nserver\n:\n\n        \nname\n:\n \ntest-server-1\n\n        \nipaddress\n:\n \n192.168.1.6\n\n\n  \ntasks\n:\n\n    \n-\n \nname\n:\n \nGet resource\n\n      \ndelegate_to\n:\n \nlocalhost\n\n      \nignore_errors\n:\n \ntrue\n\n      \nregister\n:\n \nresult\n\n      \nuri\n:\n\n        \nurl\n:\n \nhttp://{{\n \nnsip\n \n}}/nitro/v1/config/{{\n \nresource\n \n}}/{{\n \nrequest_payload.server.name\n \n}}\n\n        \nmethod\n:\n \nGET\n\n        \nstatus_code\n:\n \n200\n\n        \nreturn_content\n:\n \nyes\n\n        \nheaders\n:\n\n          \nX-NITRO-USER\n:\n \n{{\n \nnitro_user\n \n}}\n\n          \nX-NITRO-PASS\n:\n \n{{\n \nnitro_pass\n \n}}\n\n\n    \n-\n \nname\n:\n \nCheck success or expected failure\n\n      \nassert\n:\n\n        \nthat\n:\n \nresult|succeeded or ( result|failed and result.json.errorcode == 258 )\n\n\n    \n-\n \nname\n:\n \nAdd resource when not existing\n\n      \ndelegate_to\n:\n \nlocalhost\n\n      \nwhen\n:\n \nresult|failed\n\n      \nuri\n:\n\n        \nurl\n:\n \nhttp://{{\n \nnsip\n \n}}/nitro/v1/config/{{\n \nresource\n \n}}\n\n        \nmethod\n:\n \nPOST\n\n        \nstatus_code\n:\n \n201\n\n        \nreturn_content\n:\n \nyes\n\n        \nheaders\n:\n\n          \nX-NITRO-USER\n:\n \n{{\n \nnitro_user\n \n}}\n\n          \nX-NITRO-PASS\n:\n \n{{\n \nnitro_pass\n \n}}\n\n        \nbody_format\n:\n \njson\n\n        \nbody\n:\n \n{{\n \nrequest_payload\n \n}}\n\n\n    \n-\n \nname\n:\n \nUpdate resource if existing\n\n      \ndelegate_to\n:\n \nlocalhost\n\n      \nwhen\n:\n \nresult|succeeded\n\n      \nuri\n:\n\n        \nurl\n:\n \nhttp://{{\n \nnsip\n \n}}/nitro/v1/config/{{\n \nresource\n \n}}\n\n        \nmethod\n:\n \nPUT\n\n        \nstatus_code\n:\n \n200\n\n        \nreturn_content\n:\n \nyes\n\n        \nheaders\n:\n\n          \nX-NITRO-USER\n:\n \n{{\n \nnitro_user\n \n}}\n\n          \nX-NITRO-PASS\n:\n \n{{\n \nnitro_pass\n \n}}\n\n        \nbody_format\n:\n \njson\n\n        \nbody\n:\n \n{{\n \nrequest_payload\n \n}}\n\n\n    \n-\n \nname\n:\n \nSave running configuration\n\n      \ndelegate_to\n:\n \nlocalhost\n\n      \nuri\n:\n\n        \nurl\n:\n \nhttp://{{\n \nnsip\n \n}}/nitro/v1/config/nsconfig?action=save\n\n        \nmethod\n:\n \nPOST\n\n        \nstatus_code\n:\n \n200\n\n        \nheaders\n:\n\n          \nX-NITRO-USER\n:\n \n{{\n \nnitro_user\n \n}}\n\n          \nX-NITRO-PASS\n:\n \n{{\n \nnitro_pass\n \n}}\n\n        \nbody_format\n:\n \njson\n\n        \nbody\n:\n\n          \nnsconfig\n:\n \n{}\n\n\n\n\n\n\nFor the first task which detects if the resource already exists we have\nset \nignore_errors: true\n. This has the effect that an error will not\nstop the execution of the playbook. We also register the result under\nthe variable \nresult\n to be available for examination in the following\ntasks.\n\n\nThe next task leverages Ansible's \nassert module\n to\ndistinguish between an expected failure and an unexpected one. In the\ncase of an unexpected failure this task fails and prevents any further\nexecution.\n\n\nNext there are two tasks, one creating the resource and one updating the\nexisting resource. Which one executes depends on the condition defined\nin each task's \nwhen:\n option.\n\n\nReferences\n\n\n\n\nAnsible NITRO API calls repository\n\n\nAnsible uri module documentation\n\n\nAnsible assert module documentation", 
            "title": "Direct NITRO API calls"
        }, 
        {
            "location": "/generic-modules/nitro-api-calls/#direct-nitro-api-calls", 
            "text": "One method of configuring Netscaler consists of making direct NITRO API\ncalls using Ansbile's  uri module .  This method tends to be quite verbose since setting up even basic\nfunctions requires multiple NITRO calls.  Another consideration is failure robustness. NITRO API call failures\nresult in the uri module task failing immediately and stopping the\nexecution of the rest of the playbook.  This may be desired behavior in general since in some cases we need to\nexamine the failure response to actually determine if the operation was\nindeed a failure or expected.  An example of that would be trying to add a resource while it exists.\nThis is will result in failure since the HTTP POST request will not\ncreate the resource but this does not mean the configuration of\nNetscaler is necessarily invalid.  Using Ansible's conditional constructs we can work around this problem\nin most cases but this adds to the verbosity and complexity of the\nplaybooks.", 
            "title": "Direct NITRO API calls"
        }, 
        {
            "location": "/generic-modules/nitro-api-calls/#workflow", 
            "text": "In the following example we use direct NITRO API calls to create or\nupdate a basic server.  The play would be quite short but we have added some control logic to\ndetect whether the resource already exists and then apply the\nappropriate operation.  We first try to get the details of the configuration resource. We\nexamine the outcome of this operation and if it was successful we\nproceed to update the resource. If it failed we examine the exact\nerrorcode and if it signifies that the error was due to the resource\nmissing we proceed to create it.  On any other outcome, an error that was not what was expected, the play\nfails.  The final task is to save the running configuration to ensure that a\nreboot of Netscaler will not undo the changes we have made.", 
            "title": "Workflow"
        }, 
        {
            "location": "/generic-modules/nitro-api-calls/#playbook", 
            "text": "-   hosts :   netscaler \n   gather_facts :   no \n   vars : \n     resource :   server \n     request_payload : \n       server : \n         name :   test-server-1 \n         ipaddress :   192.168.1.6 \n\n   tasks : \n     -   name :   Get resource \n       delegate_to :   localhost \n       ignore_errors :   true \n       register :   result \n       uri : \n         url :   http://{{   nsip   }}/nitro/v1/config/{{   resource   }}/{{   request_payload.server.name   }} \n         method :   GET \n         status_code :   200 \n         return_content :   yes \n         headers : \n           X-NITRO-USER :   {{   nitro_user   }} \n           X-NITRO-PASS :   {{   nitro_pass   }} \n\n     -   name :   Check success or expected failure \n       assert : \n         that :   result|succeeded or ( result|failed and result.json.errorcode == 258 ) \n\n     -   name :   Add resource when not existing \n       delegate_to :   localhost \n       when :   result|failed \n       uri : \n         url :   http://{{   nsip   }}/nitro/v1/config/{{   resource   }} \n         method :   POST \n         status_code :   201 \n         return_content :   yes \n         headers : \n           X-NITRO-USER :   {{   nitro_user   }} \n           X-NITRO-PASS :   {{   nitro_pass   }} \n         body_format :   json \n         body :   {{   request_payload   }} \n\n     -   name :   Update resource if existing \n       delegate_to :   localhost \n       when :   result|succeeded \n       uri : \n         url :   http://{{   nsip   }}/nitro/v1/config/{{   resource   }} \n         method :   PUT \n         status_code :   200 \n         return_content :   yes \n         headers : \n           X-NITRO-USER :   {{   nitro_user   }} \n           X-NITRO-PASS :   {{   nitro_pass   }} \n         body_format :   json \n         body :   {{   request_payload   }} \n\n     -   name :   Save running configuration \n       delegate_to :   localhost \n       uri : \n         url :   http://{{   nsip   }}/nitro/v1/config/nsconfig?action=save \n         method :   POST \n         status_code :   200 \n         headers : \n           X-NITRO-USER :   {{   nitro_user   }} \n           X-NITRO-PASS :   {{   nitro_pass   }} \n         body_format :   json \n         body : \n           nsconfig :   {}   For the first task which detects if the resource already exists we have\nset  ignore_errors: true . This has the effect that an error will not\nstop the execution of the playbook. We also register the result under\nthe variable  result  to be available for examination in the following\ntasks.  The next task leverages Ansible's  assert module  to\ndistinguish between an expected failure and an unexpected one. In the\ncase of an unexpected failure this task fails and prevents any further\nexecution.  Next there are two tasks, one creating the resource and one updating the\nexisting resource. Which one executes depends on the condition defined\nin each task's  when:  option.", 
            "title": "Playbook"
        }, 
        {
            "location": "/generic-modules/nitro-api-calls/#references", 
            "text": "Ansible NITRO API calls repository  Ansible uri module documentation  Ansible assert module documentation", 
            "title": "References"
        }, 
        {
            "location": "/development-utilities/", 
            "text": "Development Utilities\n\n\nUnder the utils/ directory there exists a number of scripts and data files that aid in the development process.\n\n\nBe advised that the state of the scripts is always in flux, so this documentation page may lag behind the actual implementation at times.\n\n\nDeveloping a new module\n\n\nThere is a lot of boilerplate code that goes into each module since the workflow is roughly similar for configuring a resource using a singular NITRO object.\n\n\nTo aid with this there are some scripts under utils/ to aid with the generation of this code.\n\n\nThe parts that are not covered by the boilerplate generation code is peculiarities of each NITRO object. For example having to use a different nitro object to add/update the resource and a different object to determine its existance and configuration parameters.\n\n\nAlso when adding bindings to an object there is some manual work to be done to configure how the bindings tie in with the main object and to maintain the correct control flow of the module. Still in this cases generating a module file for the binding may be beneficial since some parts of the generated module can be copied to the more complex one that uses that object combined with that module\u2019s main nitro object.\n\n\nGetting the spec of a nitro object\n\n\nTo get the specification of a nitro object there is a script named scrape.py.\n\n\nThis script scrapes the nitro reference web site for each object defined in a hardcoded list and produces for each page scraped a json file.\n\n\nThis json file contains information about the properties of the nitro object and is used by subsequent scripts.\n\n\nThe operation of scrape.py is based on parsing the HTML DOM for each page and may fail for some nitro objects.\n\n\nTo add an object to be scrapped just edit the hardcoded list of objects and run the script.\n\n\nGenerating the boilerplate\n\n\nTo generate the boilerplate the script compile.py has to be called.\n\n\nThis script has a hardcoded list of objects and generates for each an initial version of the corresponding module.\n\n\nThis script has as input for each nitro object the json data file which was obtained by the scrape.py script and the actual class of the Python NITRO SDK that corresponds to this object. The Python SDK must be importable when this script is run.\n\n\nThe script checks if there are differences between the attributes defined in the SDK and the attributes from the scraped json and will output warnings for each attribute missing. The attributes that will go into the generated code will be the ones present in both the SDK object and the json data file.\n\n\nThe json data file must be under utils/source/scrape.\n\n\nThe output python file is put under utils/output\n\n\nThe generated code contains the documentation for the attributes of the nitro object, the instantiation of a ConfigProxy object for the object and the control flow statements for the main module execution. Placeholders are marked by a single underscore \u201c_\u201d or names that start with a single underscore.\n\n\nReplacing the placeholders, implementing the object bindings if there are any, and verifying and correcting the control flow are the most common manual steps that follow.", 
            "title": "Developer Documentation"
        }, 
        {
            "location": "/development-utilities/#development-utilities", 
            "text": "Under the utils/ directory there exists a number of scripts and data files that aid in the development process.  Be advised that the state of the scripts is always in flux, so this documentation page may lag behind the actual implementation at times.", 
            "title": "Development Utilities"
        }, 
        {
            "location": "/development-utilities/#developing-a-new-module", 
            "text": "There is a lot of boilerplate code that goes into each module since the workflow is roughly similar for configuring a resource using a singular NITRO object.  To aid with this there are some scripts under utils/ to aid with the generation of this code.  The parts that are not covered by the boilerplate generation code is peculiarities of each NITRO object. For example having to use a different nitro object to add/update the resource and a different object to determine its existance and configuration parameters.  Also when adding bindings to an object there is some manual work to be done to configure how the bindings tie in with the main object and to maintain the correct control flow of the module. Still in this cases generating a module file for the binding may be beneficial since some parts of the generated module can be copied to the more complex one that uses that object combined with that module\u2019s main nitro object.", 
            "title": "Developing a new module"
        }, 
        {
            "location": "/development-utilities/#getting-the-spec-of-a-nitro-object", 
            "text": "To get the specification of a nitro object there is a script named scrape.py.  This script scrapes the nitro reference web site for each object defined in a hardcoded list and produces for each page scraped a json file.  This json file contains information about the properties of the nitro object and is used by subsequent scripts.  The operation of scrape.py is based on parsing the HTML DOM for each page and may fail for some nitro objects.  To add an object to be scrapped just edit the hardcoded list of objects and run the script.", 
            "title": "Getting the spec of a nitro object"
        }, 
        {
            "location": "/development-utilities/#generating-the-boilerplate", 
            "text": "To generate the boilerplate the script compile.py has to be called.  This script has a hardcoded list of objects and generates for each an initial version of the corresponding module.  This script has as input for each nitro object the json data file which was obtained by the scrape.py script and the actual class of the Python NITRO SDK that corresponds to this object. The Python SDK must be importable when this script is run.  The script checks if there are differences between the attributes defined in the SDK and the attributes from the scraped json and will output warnings for each attribute missing. The attributes that will go into the generated code will be the ones present in both the SDK object and the json data file.  The json data file must be under utils/source/scrape.  The output python file is put under utils/output  The generated code contains the documentation for the attributes of the nitro object, the instantiation of a ConfigProxy object for the object and the control flow statements for the main module execution. Placeholders are marked by a single underscore \u201c_\u201d or names that start with a single underscore.  Replacing the placeholders, implementing the object bindings if there are any, and verifying and correcting the control flow are the most common manual steps that follow.", 
            "title": "Generating the boilerplate"
        }
    ]
}